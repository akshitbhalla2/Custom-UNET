{"cells":[{"metadata":{"id":"PoNSZn1D7d8J"},"cell_type":"markdown","source":"# Semantic Segmentation with tf.data in TensorFlow 2 and ADE20K dataset"},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction"},{"metadata":{"id":"4KnA3joV7nVR"},"cell_type":"markdown","source":"In this notebook we are going to cover the usage of tensorflow 2 and tf.data on a popular semantic segmentation 2D images dataset: ADE20K.\n\nThe type of data we are going to manipulate consist in:\n* an jpg image with 3 channels (RGB)\n* a jpg mask with 1 channel (for each pixel we have 1 true class over 150 possible)  \n\nYou can also find all the information by reading the official tensorflow tutorials:\n\n* https://www.tensorflow.org/tutorials/load_data/images\n* https://www.tensorflow.org/tutorials/images/segmentation"},{"metadata":{},"cell_type":"markdown","source":"This notebook assumes that you already dowwnloaded ADE20k and extracted the content of the archive in `./data/ADEChallengeData2016/` with `./` meaning the directory where this notebook is running.  \nYou can download it there: http://sceneparsing.csail.mit.edu/  \nAlso, if you run this notebook, a GPU is almost mandatory since the computations take A LOT of time on CPU."},{"metadata":{"id":"53uMkooT79_4"},"cell_type":"markdown","source":"## 2. Preparing the Environment"},{"metadata":{"id":"0lCFAUR97ZDR","trusted":true},"cell_type":"code","source":"from glob import glob\n\nimport IPython.display as display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport datetime, os\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom IPython.display import clear_output\nimport tensorflow_addons as tfa\n\n# For more information about autotune:\n# https://www.tensorflow.org/guide/data_performance#prefetching\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nprint(f\"Tensorflow ver. {tf.__version__}\")","execution_count":13,"outputs":[{"output_type":"stream","text":"Tensorflow ver. 2.3.1\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n The versions of TensorFlow you are currently using is 2.3.1 and is not supported. \nSome things might work, some things might not.\nIf you were to encounter a bug, do not file an issue.\nIf you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \nYou can find the compatibility matrix in TensorFlow Addon's readme:\nhttps://github.com/tensorflow/addons\n  UserWarning,\n","name":"stderr"}]},{"metadata":{"id":"pGKYiwWM7my6","trusted":true},"cell_type":"code","source":"# important for reproducibility\n# this allows to generate the same random numbers\nSEED = 42","execution_count":14,"outputs":[]},{"metadata":{"id":"VW3jgp_1_q-6","trusted":true},"cell_type":"code","source":"# you can change this path to reflect your own settings if necessary\ndataset_path = \"../input/ade20k/ADE20K_2016_07_26/images/\"\ntraining_data = \"training/\"\nval_data = \"validation/\"","execution_count":15,"outputs":[]},{"metadata":{"id":"cG3cAd8U8mj9"},"cell_type":"markdown","source":"By default, tensorflow uses 100% of the available GPU memory. It allows to do contiguous memory allocation with is potentially faster. You can deactivate this default behavior. I personally feel useful the fact that I can check how much memory is used considering the batch size."},{"metadata":{"trusted":true},"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)","execution_count":16,"outputs":[{"output_type":"stream","text":"1 Physical GPUs, 1 Logical GPUs\n","name":"stdout"}]},{"metadata":{"id":"yocgxYITGXSv"},"cell_type":"markdown","source":"## 3. Creating our Dataloader"},{"metadata":{"id":"-KqCjhW6-Rdl","trusted":true},"cell_type":"code","source":"# Image size that we are going to use\nIMG_SIZE = 128\n# Our images are RGB (3 channels)\nN_CHANNELS = 3\n# Scene Parsing has 150 classes + `not labeled`\nN_CLASSES = 151","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1. Creating a source dataset"},{"metadata":{"id":"7znO6EafQs8S","outputId":"816b761b-2967-45b6-ef02-d4cffbfe17c4","trusted":true},"cell_type":"code","source":"TRAINSET_SIZE = len(glob(dataset_path + training_data + \"*/*/*.jpg\"))\nprint(f\"The Training Dataset contains {TRAINSET_SIZE} images.\")\n\nVALSET_SIZE = len(glob(dataset_path + val_data + \"*/*/*.jpg\"))\nprint(f\"The Validation Dataset contains {VALSET_SIZE} images.\")","execution_count":18,"outputs":[{"output_type":"stream","text":"The Training Dataset contains 0 images.\nThe Validation Dataset contains 0 images.\n","name":"stdout"}]},{"metadata":{"id":"S3IeIvttGer6"},"cell_type":"markdown","source":"For each images of our dataset, we will apply some operations wrapped into a function. Then we will map the whole dataset with this function.   \n\nSo let's write this function:"},{"metadata":{"id":"REeU48WsGub0","trusted":true},"cell_type":"code","source":"def parse_image(img_path: str) -> dict:\n    \"\"\"Load an image and its annotation (mask) and returning\n    a dictionary.\n\n    Parameters\n    ----------\n    img_path : str\n        Image (not the mask) location.\n\n    Returns\n    -------\n    dict\n        Dictionary mapping an image and its annotation.\n    \"\"\"\n    image = tf.io.read_file(img_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.uint8)\n\n    # For one Image path:\n    # .../trainset/images/training/ADE_train_00000001.jpg\n    # Its corresponding annotation path is:\n    # .../trainset/annotations/training/ADE_train_00000001.png\n    mask_path = tf.strings.regex_replace(img_path, \"images\", \"annotations\")\n    mask_path = tf.strings.regex_replace(mask_path, \"jpg\", \"png\")\n    mask = tf.io.read_file(mask_path)\n    # The masks contain a class index for each pixels\n    mask = tf.image.decode_png(mask, channels=1)\n    # In scene parsing, \"not labeled\" = 255\n    # But it will mess up with our N_CLASS = 150\n    # Since 255 means the 255th class\n    # Which doesn't exist\n    mask = tf.where(mask == 255, np.dtype('uint8').type(0), mask)\n    # Note that we have to convert the new value (0)\n    # With the same dtype than the tensor itself\n\n    return {'image': image, 'segmentation_mask': mask}","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In case you would like to load any other image format, you should modify the `parse_image` function. TensorFlow I/O provide additional tools that might help you. \nFor example, in the case of loading TIFF images, you can use:\n\n```python\nimport tensorflow as tf\nimport tensorflow.io as tfio\n...\ndef parse_image(img_path: str) -> dict:\n...\nimage = tf.io.read_file(img_path)\ntfio.experimental.image.decode_tiff(image)\n...\n```\n\nIn this case, don't forget to modify the number of channels when implementing the model later."},{"metadata":{"id":"6Vpo8nhYGwy6","trusted":true},"cell_type":"code","source":"train_dataset = tf.data.Dataset.list_files(dataset_path + training_data + \"*/*/*.jpg\", seed=SEED)\ntrain_dataset = train_dataset.map(parse_image)\n\nval_dataset = tf.data.Dataset.list_files(dataset_path + val_data + \"*/*/*.jpg\", seed=SEED)\nval_dataset =val_dataset.map(parse_image)","execution_count":41,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2. Applying some transformations to our dataset"},{"metadata":{"id":"S0Pa1mgPSSOL","trusted":true},"cell_type":"code","source":"# Here we are using the decorator @tf.function\n# if you want to know more about it:\n# https://www.tensorflow.org/api_docs/python/tf/function\n\n@tf.function\ndef normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n    \"\"\"Rescale the pixel values of the images between 0.0 and 1.0\n    compared to [0,255] originally.\n\n    Parameters\n    ----------\n    input_image : tf.Tensor\n        Tensorflow tensor containing an image of size [SIZE,SIZE,3].\n    input_mask : tf.Tensor\n        Tensorflow tensor containing an annotation of size [SIZE,SIZE,1].\n\n    Returns\n    -------\n    tuple\n        Normalized image and its annotation.\n    \"\"\"\n    input_image = tf.cast(input_image, tf.float32) / 255.0\n    return input_image, input_mask\n\n@tf.function\ndef load_image_train(datapoint: dict) -> tuple:\n    \"\"\"Apply some transformations to an input dictionary\n    containing a train image and its annotation.\n\n    Notes\n    -----\n    An annotation is a regular  channel image.\n    If a transformation such as rotation is applied to the image,\n    the same transformation has to be applied on the annotation also.\n\n    Parameters\n    ----------\n    datapoint : dict\n        A dict containing an image and its annotation.\n\n    Returns\n    -------\n    tuple\n        A modified image and its annotation.\n    \"\"\"\n    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n\n    if tf.random.uniform(()) > 0.5:\n        input_image = tf.image.flip_left_right(input_image)\n        input_mask = tf.image.flip_left_right(input_mask)\n\n    input_image, input_mask = normalize(input_image, input_mask)\n\n    return input_image, input_mask\n\n@tf.function\ndef load_image_test(datapoint: dict) -> tuple:\n    \"\"\"Normalize and resize a test image and its annotation.\n\n    Notes\n    -----\n    Since this is for the test set, we don't need to apply\n    any data augmentation technique.\n\n    Parameters\n    ----------\n    datapoint : dict\n        A dict containing an image and its annotation.\n\n    Returns\n    -------\n    tuple\n        A modified image and its annotation.\n    \"\"\"\n    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n\n    input_image, input_mask = normalize(input_image, input_mask)\n\n    return input_image, input_mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Manipulating datasets in tensorflow can be complicated. You can read the official documentation to understand how they are working: https://www.tensorflow.org/guide/data#training_workflows"},{"metadata":{"id":"J0BpQ8uwHXwb","outputId":"1051e4aa-6daa-4148-e2a1-f8ca2c9b813c","trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\n\n# for reference about the BUFFER_SIZE in shuffle:\n# https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\nBUFFER_SIZE = 1000\n\ndataset = {\"train\": train_dataset, \"val\": val_dataset}\n\n# -- Train Dataset --#\ndataset['train'] = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ndataset['train'] = dataset['train'].shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\ndataset['train'] = dataset['train'].repeat()\ndataset['train'] = dataset['train'].batch(BATCH_SIZE)\ndataset['train'] = dataset['train'].prefetch(buffer_size=AUTOTUNE)\n\n#-- Validation Dataset --#\ndataset['val'] = dataset['val'].map(load_image_test)\ndataset['val'] = dataset['val'].repeat()\ndataset['val'] = dataset['val'].batch(BATCH_SIZE)\ndataset['val'] = dataset['val'].prefetch(buffer_size=AUTOTUNE)\n\nprint(dataset['train'])\nprint(dataset['val'])\n\n# how shuffle works: https://stackoverflow.com/a/53517848","execution_count":null,"outputs":[]},{"metadata":{"id":"QeICDrVjS4ig"},"cell_type":"markdown","source":"## 4. Visualizing the Dataset"},{"metadata":{},"cell_type":"markdown","source":"It seems everything is fine. It can be very hard to build your model by having bugs in your dataset. This makes the development process very painful since the potential bugs from your model are adding up to the potential bugs in your dataloaders. Therefore, it is recommended to make sure that you have what you expect.  \nFor that, we are going to develop simple functions to vizualize the content of our dataloaders."},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_sample(display_list):\n    \"\"\"Show side-by-side an input image,\n    the ground truth and the prediction.\n    \"\"\"\n    plt.figure(figsize=(18, 18))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image, mask in dataset['train'].take(1):\n    sample_image, sample_mask = image, mask\n\ndisplay_sample([sample_image[0], sample_mask[0]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dimensions are the ones we expect and it shows up properly. We can start the development of the model itself."},{"metadata":{},"cell_type":"markdown","source":"## 5. Developing the Model (UNet) Using Keras Functional API\nFor this example, we are going to implement a popular architecture: UNet. In a sense, it is not the best for a titorial since this model is very heavy. But I found the exercise interesting. Especially because we are going to use the functional API provided by keras.\n\nThis architecture was introduce in the paper **U-Net: Convolutional Networks for Biomedical Image Segmentation** that you can read there: https://arxiv.org/abs/1505.04597  \n\nYou can also read my notes on this paper there: https://yann-leguilly.gitlab.io/post/2019-12-11-unet-biomedical-images/   \nBasically we are going to reproduce this:  \n\n![figure 1](https://yann-leguilly.gitlab.io/img/unet_1/figure_1.png)"},{"metadata":{},"cell_type":"markdown","source":"### 5.1. Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Keras Functional API -- #\n# -- UNet Implementation -- #\n# Everything here is from tensorflow.keras.layers\n# I imported tensorflow.keras.layers * to make it easier to read\ndropout_rate = 0.5\ninput_size = (IMG_SIZE, IMG_SIZE, N_CHANNELS)\n\n# If you want to know more about why we are using `he_normal`: \n# https://stats.stackexchange.com/questions/319323/whats-the-difference-between-variance-scaling-initializer-and-xavier-initialize/319849#319849  \n# Or the excelent fastai course: \n# https://github.com/fastai/course-v3/blob/master/nbs/dl2/02b_initializing.ipynb\ninitializer = 'he_normal'\n\n\n# -- Encoder -- #\n# Block encoder 1\ninputs = Input(shape=input_size)\nconv_enc_1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=initializer)(inputs)\nconv_enc_1 = Conv2D(64, 3, activation = 'relu', padding='same', kernel_initializer=initializer)(conv_enc_1)\n\n# Block encoder 2\nmax_pool_enc_2 = MaxPooling2D(pool_size=(2, 2))(conv_enc_1)\nconv_enc_2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(max_pool_enc_2)\nconv_enc_2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(conv_enc_2)\n\n# Block  encoder 3\nmax_pool_enc_3 = MaxPooling2D(pool_size=(2, 2))(conv_enc_2)\nconv_enc_3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(max_pool_enc_3)\nconv_enc_3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(conv_enc_3)\n\n# Block  encoder 4\nmax_pool_enc_4 = MaxPooling2D(pool_size=(2, 2))(conv_enc_3)\nconv_enc_4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(max_pool_enc_4)\nconv_enc_4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(conv_enc_4)\n# -- Encoder -- #\n\n# ----------- #\nmaxpool = MaxPooling2D(pool_size=(2, 2))(conv_enc_4)\nconv = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(maxpool)\nconv = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(conv)\n# ----------- #\n\n# -- Dencoder -- #\n# Block decoder 1\nup_dec_1 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = initializer)(UpSampling2D(size = (2,2))(conv))\nmerge_dec_1 = concatenate([conv_enc_4, up_dec_1], axis = 3)\nconv_dec_1 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(merge_dec_1)\nconv_dec_1 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(conv_dec_1)\n\n# Block decoder 2\nup_dec_2 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = initializer)(UpSampling2D(size = (2,2))(conv_dec_1))\nmerge_dec_2 = concatenate([conv_enc_3, up_dec_2], axis = 3)\nconv_dec_2 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(merge_dec_2)\nconv_dec_2 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(conv_dec_2)\n\n# Block decoder 3\nup_dec_3 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = initializer)(UpSampling2D(size = (2,2))(conv_dec_2))\nmerge_dec_3 = concatenate([conv_enc_2, up_dec_3], axis = 3)\nconv_dec_3 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(merge_dec_3)\nconv_dec_3 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(conv_dec_3)\n\n# Block decoder 4\nup_dec_4 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = initializer)(UpSampling2D(size = (2,2))(conv_dec_3))\nmerge_dec_4 = concatenate([conv_enc_1, up_dec_4], axis = 3)\nconv_dec_4 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(merge_dec_4)\nconv_dec_4 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(conv_dec_4)\nconv_dec_4 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)(conv_dec_4)\n# -- Dencoder -- #\n\noutput = Conv2D(N_CLASSES, 1, activation = 'softmax')(conv_dec_4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can load and compile the model to make sure that there is no bugs."},{"metadata":{"id":"Sfh1cVtt1Y12","trusted":true},"cell_type":"code","source":"model = tf.keras.Model(inputs = inputs, outputs = output)","execution_count":null,"outputs":[]},{"metadata":{"id":"TlAIZzR600uK","trusted":true},"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.0001), loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"_ekf2x5FS_df"},"cell_type":"markdown","source":"### 5.2. Sanity Check"},{"metadata":{},"cell_type":"markdown","source":"We can visualize a sample prediction to be sure that we see what we expect."},{"metadata":{"id":"RCeYBs5ldoJx","outputId":"be3cf105-6c61-4e8b-d55b-3ae06f98da7f","trusted":true},"cell_type":"code","source":"def create_mask(pred_mask: tf.Tensor) -> tf.Tensor:\n    \"\"\"Return a filter mask with the top 1 predicitons\n    only.\n\n    Parameters\n    ----------\n    pred_mask : tf.Tensor\n        A [IMG_SIZE, IMG_SIZE, N_CLASS] tensor. For each pixel we have\n        N_CLASS values (vector) which represents the probability of the pixel\n        being these classes. Example: A pixel with the vector [0.0, 0.0, 1.0]\n        has been predicted class 2 with a probability of 100%.\n\n    Returns\n    -------\n    tf.Tensor\n        A [IMG_SIZE, IMG_SIZE, 1] mask with top 1 predictions\n        for each pixels.\n    \"\"\"\n    # pred_mask -> [IMG_SIZE, SIZE, N_CLASS]\n    # 1 prediction for each class but we want the highest score only\n    # so we use argmax\n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    # pred_mask becomes [IMG_SIZE, IMG_SIZE]\n    # but matplotlib needs [IMG_SIZE, IMG_SIZE, 1]\n    pred_mask = tf.expand_dims(pred_mask, axis=-1)\n    return pred_mask\n    \ndef show_predictions(dataset=None, num=1):\n    \"\"\"Show a sample prediction.\n\n    Parameters\n    ----------\n    dataset : [type], optional\n        [Input dataset, by default None\n    num : int, optional\n        Number of sample to show, by default 1\n    \"\"\"\n    if dataset:\n        for image, mask in dataset.take(num):\n            pred_mask = model.predict(image)\n            display_sample([image[0], true_mask, create_mask(pred_mask)])\n    else:\n        # The model is expecting a tensor of the size\n        # [BATCH_SIZE, IMG_SIZE, IMG_SIZE, 3]\n        # but sample_image[0] is [IMG_SIZE, IMG_SIZE, 3]\n        # and we want only 1 inference to be faster\n        # so we add an additional dimension [1, IMG_SIZE, IMG_SIZE, 3]\n        one_img_batch = sample_image[0][tf.newaxis, ...]\n        # one_img_batch -> [1, IMG_SIZE, IMG_SIZE, 3]\n        inference = model.predict(one_img_batch)\n        # inference -> [1, IMG_SIZE, IMG_SIZE, N_CLASS]\n        pred_mask = create_mask(inference)\n        # pred_mask -> [1, IMG_SIZE, IMG_SIZE, 1]\n        display_sample([sample_image[0], sample_mask[0],\n                        pred_mask[0]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image, mask in dataset['train'].take(1):\n    sample_image, sample_mask = image, mask\n\nshow_predictions()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The far right image is a prediction with random weights. We are ready to start the training!"},{"metadata":{},"cell_type":"markdown","source":"## 6. Training our Model"},{"metadata":{},"cell_type":"markdown","source":"### 6.1. Simpler training loop"},{"metadata":{},"cell_type":"markdown","source":"Let's run a simple training loop with only 1 epoch first. "},{"metadata":{"id":"Lg8SkdewgOiP","trusted":true},"cell_type":"code","source":"EPOCHS = 1\n\nSTEPS_PER_EPOCH = TRAINSET_SIZE // BATCH_SIZE\nVALIDATION_STEPS = VALSET_SIZE // BATCH_SIZE","execution_count":null,"outputs":[]},{"metadata":{"id":"qCUBmThngQ0g","trusted":true},"cell_type":"code","source":"# sometimes it can be very interesting to run some batches on cpu\n# because the tracing is way better than on GPU\n# you will have more obvious error message\n# but in our case, it takes A LOT of time\n\n# On CPU\n# with tf.device(\"/cpu:0\"):\n#     model_history = model.fit(dataset['train'], epochs=EPOCHS,\n#                               steps_per_epoch=STEPS_PER_EPOCH,\n#                               validation_steps=VALIDATION_STEPS,\n#                               validation_data=dataset['val'])\n\n# On GPU\nmodel_history = model.fit(dataset['train'], epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=dataset['val'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Everything looks great! We can start our 'real' training."},{"metadata":{},"cell_type":"markdown","source":"### 6.2. More Advanced Training Loop"},{"metadata":{},"cell_type":"markdown","source":"Keras implements what we call 'callbacks'. We can use them to run custom functions at any step of the training. Here we are going to show the output of the model compared to the original image and the ground truth after each epochs.\nWe are also going to collect some useful metrics to make sure our training is happening well by using tensorboard."},{"metadata":{"id":"zten6PA0t_fQ","trusted":true},"cell_type":"code","source":"class DisplayCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        clear_output(wait=True)\n        show_predictions()\n        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))","execution_count":null,"outputs":[]},{"metadata":{"id":"Wg_kxEQef_or","trusted":true},"cell_type":"code","source":"EPOCHS = 20\n\nlogdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n\ncallbacks = [\n    # to show samples after each epoch\n    DisplayCallback(),\n    # to collect some useful metrics and visualize them in tensorboard\n    tensorboard_callback,\n    # if no accuracy improvements we can stop the training directly\n    tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n    # to save checkpoints\n    tf.keras.callbacks.ModelCheckpoint('best_model_unet.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]\n\nmodel = tf.keras.Model(inputs = inputs, outputs = output)\n\n# # here I'm using a new optimizer: https://arxiv.org/abs/1908.03265\noptimizer=tfa.optimizers.RectifiedAdam(lr=1e-3)\n\nloss = tf.keras.losses.SparseCategoricalCrossentropy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=optimizer, loss = loss,\n                  metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = train_model(model, optimizer, loss, callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}